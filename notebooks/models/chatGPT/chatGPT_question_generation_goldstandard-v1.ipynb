{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## ChatGPT model evaluation on Goldstandard-V2 dataset\n",
    "This notebook is used to experiment with different prompts for the question generation model that uses the chatGPT API from open.ai.\n",
    "First a helper function is created to call the API with the provided prompt. For this prompt different techniques are tried out and evaluated to find the best performing prompt template."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-15T20:57:23.643929Z",
     "start_time": "2023-07-15T20:57:10.353492Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/I516258/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from src.datageneration.extractor import extract_text_without_image\n",
    "from pypdfium2 import PdfDocument\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.evaluation.eval_main import Metrics\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI-API-KEY\")\n",
    "\n",
    "def chat_gpt(prompt, temperature=0):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we prepare the data for the evaluation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # initially retrieve extracted text for each slide - only execute once\n",
    "slide_path = \"../../../datasets/IT-Security_all_slides_no_duplicates.pdf\"\n",
    "pdf = PdfDocument(slide_path)\n",
    "text = extract_text_without_image(pdf.raw)\n",
    "extracted_content = pd.DataFrame(columns=['Pagenumber', 'Page-Text', 'OCR-text'])\n",
    "for i in text:\n",
    "    extracted_content = extracted_content.append({'Pagenumber': i[0], 'Page-Text': i[1], 'OCR-text': i[2]}, ignore_index=True)\n",
    "\n",
    "# Define the file path and name\n",
    "file_path = \"../../../datasets/extracted_text_content.csv\"\n",
    "\n",
    "# Save the DataFrame to the specified folder\n",
    "extracted_content.to_csv(file_path, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T20:47:53.507464Z",
     "start_time": "2023-07-15T20:42:09.018872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pagenumber                                          Page-Text  \\\n",
      "0             0  Selected Topics in IT-Security\\r\\nProf. Dr. Fr...   \n",
      "1             1  Simple Model\\r\\n1. User(s)\\r\\n• Access the sys...   \n",
      "2             2  IT-Security\\r\\n• Security (german: Sicherheit)...   \n",
      "3             3  Attacker - Examples\\r\\n• National agency\\r\\n• ...   \n",
      "4             4  Attacker Model\\r\\n• Usually specifies what the...   \n",
      "..          ...                                                ...   \n",
      "591         591  Differential Privacy\\r\\nIntuition\\r\\n• Assume ...   \n",
      "592         592  Differential Privacy\\r\\nDefinition (Simplified...   \n",
      "593         593  On the Parameter \\r\\nPr  ଵ =  ≤ ఢ ⋅ Pr  ଶ = \\r...   \n",
      "594         594  Privacy Budget\\r\\n• Defines an upper bound on ...   \n",
      "595         595  Making Algorithms Differentially Private\\r\\n• ...   \n",
      "\n",
      "                                              OCR-text  \n",
      "0    Fealitet\\nSelected Topics in IT-Security BB OF...  \n",
      "1    Simple Model\\n\\n1. User(s)\\n\\n* Access the sys...  \n",
      "2    rs\\nF UNIVERSITY\\nIT-Security BOR MANNHEIM\\n——...  \n",
      "3    ol\\nAttacker - Examples SB OF MANNHEIM\\n——— Sc...  \n",
      "4    te\\nAttacker Model Be OP MANNHEIM\\n\\n—— School...  \n",
      "..                                                 ...  \n",
      "591  te\\nGee 5 UNIVERSITY\\n\\nDifferential Privacy 8...  \n",
      "592  ol\\nDifferential Privacy Be) OF MANNHEIM\\n\\n— ...  \n",
      "593  te\\n\\nSeta UNIVERSITY\\nOn the Parameter €-2o B...  \n",
      "594  Privacy Budget\\n\\n* Defines an upper bound on ...  \n",
      "595  ot\\nMaking Algorithms Differentially Private S...  \n",
      "\n",
      "[596 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# reload extracted content from file\n",
    "file_path = \"../../../datasets/extracted_text_content.csv\"\n",
    "extracted_content = pd.read_csv(file_path)\n",
    "\n",
    "print(extracted_content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T20:57:23.682075Z",
     "start_time": "2023-07-15T20:57:23.645874Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of test set:  91\n",
      "     Pagenumber                                          Page-Text  \\\n",
      "452         452  Cookies\\r\\nAdvantages and Disadvantages\\r\\nAdv...   \n",
      "46           46  Access Control\\r\\n• Controls which authenticat...   \n",
      "475         475  XSS\\r\\n• XSS = Cross Site Scripting\\r\\n• One o...   \n",
      "471         471  Javascript\\r\\nAbilities\\r\\n• Runs on the clien...   \n",
      "200         200  Technique\\r\\n• Recall that e-mail communicatio...   \n",
      "..          ...                                                ...   \n",
      "591         591  Differential Privacy\\r\\nIntuition\\r\\n• Assume ...   \n",
      "177         177  Reference Models for Computer \\r\\nNetworks\\r\\n...   \n",
      "108         108  Passphrases\\r\\n• Good method: choose a (silly)...   \n",
      "66           66  Some Comments\\r\\n• RBAC can be based on access...   \n",
      "199         199  E-Mail Spoofing\\r\\n• Creation of email message...   \n",
      "\n",
      "                                              OCR-text  \\\n",
      "452  Cookies\\nAdvantages and Disadvantages\\n\\nAdvan...   \n",
      "46   te\\nAccess Control Be OP MANNHEIM\\n\\n—— School...   \n",
      "475  XSS\\n\\n¢ XSS = Cross Site Scripting\\n* One of ...   \n",
      "471  ol\\nH UNIVERSITY\\nJavascript BOR MANNHEIM\\n———...   \n",
      "200  te\\nae UNIVERSITY\\n2) OF MANNHEIM\\n\\n— School ...   \n",
      "..                                                 ...   \n",
      "591  te\\nGee 5 UNIVERSITY\\n\\nDifferential Privacy 8...   \n",
      "177  alt\\nReference Models for Computer BB Oh MANNG...   \n",
      "108  ol\\nPassphrases Be) OF MANNHEIM\\n\\n—— School o...   \n",
      "66   te\\nSome Comments Be OP MANNHEIM\\n\\n—— School ...   \n",
      "199  ol\\nE-Mail Spoofing Be) OF MANNHEIM\\n——— Schoo...   \n",
      "\n",
      "                                              Question  \n",
      "452  What are Advantages and Disadvantages of Cookies?  \n",
      "46                      How is Access Control defined?  \n",
      "475                                       What is XSS?  \n",
      "471              What are the Abilities of JavaScript?  \n",
      "200                  What Spoofing Technique do exist?  \n",
      "..                                                 ...  \n",
      "591     What is the intuition of Differential Privacy?  \n",
      "177  What are the two most relevant reference model...  \n",
      "108  How do Passphrases work and what is the advant...  \n",
      "66   What advantages does RBAC have (in contrast to...  \n",
      "199                           What is E-Mail Spoofing?  \n",
      "\n",
      "[91 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = '../../../datasets/Goldstandard.csv'\n",
    "\n",
    "goldstandard = pd.read_csv(file_path, delimiter=\";\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "goldstandard.drop(['PDF-Name', 'Comment','Page Number'], axis=1, inplace=True)\n",
    "\n",
    "# Join two DataFrames based on index\n",
    "goldstandard = extracted_content.join(goldstandard, lsuffix='_left', rsuffix='_right')\n",
    "\n",
    "# Delete records with value \"No\" and \"no\" in the \"Marked for processing\" column\n",
    "goldstandard = goldstandard[(goldstandard['Marked for processing'] != 'No')]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "goldstandard.drop(['Marked for processing', 'Includes Image Data'], axis=1, inplace=True)\n",
    "\n",
    "# Split the DataFrame into train, validation, and test sets\n",
    "goldstandard_train_val, goldstandard_test = train_test_split(goldstandard, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Lenght of test set: \", len(goldstandard_test))\n",
    "print(goldstandard_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T20:57:23.705275Z",
     "start_time": "2023-07-15T20:57:23.668105Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Reset the index of the DataFrame\n",
    "goldstandard_test = goldstandard_test.reset_index(drop=True)\n",
    "goldstandard_train_val = goldstandard_train_val.reset_index(drop=True)\n",
    "\n",
    "# this stores now the possible input for the chatGPT model\n",
    "content = goldstandard_test[[\"Page-Text\", \"OCR-text\"]]\n",
    "\n",
    "# this stores the reference\n",
    "references = goldstandard_test[[\"Question\"]]\n",
    "\n",
    "references.to_csv(\"./refs.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T20:57:23.705712Z",
     "start_time": "2023-07-15T20:57:23.692441Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "91"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(references)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T20:57:23.885998Z",
     "start_time": "2023-07-15T20:57:23.871354Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            Page-Text  \\\n0   Cookies\\r\\nAdvantages and Disadvantages\\r\\nAdv...   \n1   Access Control\\r\\n• Controls which authenticat...   \n2   XSS\\r\\n• XSS = Cross Site Scripting\\r\\n• One o...   \n3   Javascript\\r\\nAbilities\\r\\n• Runs on the clien...   \n4   Technique\\r\\n• Recall that e-mail communicatio...   \n..                                                ...   \n86  Differential Privacy\\r\\nIntuition\\r\\n• Assume ...   \n87  Reference Models for Computer \\r\\nNetworks\\r\\n...   \n88  Passphrases\\r\\n• Good method: choose a (silly)...   \n89  Some Comments\\r\\n• RBAC can be based on access...   \n90  E-Mail Spoofing\\r\\n• Creation of email message...   \n\n                                             OCR-text  \n0   Cookies\\nAdvantages and Disadvantages\\n\\nAdvan...  \n1   te\\nAccess Control Be OP MANNHEIM\\n\\n—— School...  \n2   XSS\\n\\n¢ XSS = Cross Site Scripting\\n* One of ...  \n3   ol\\nH UNIVERSITY\\nJavascript BOR MANNHEIM\\n———...  \n4   te\\nae UNIVERSITY\\n2) OF MANNHEIM\\n\\n— School ...  \n..                                                ...  \n86  te\\nGee 5 UNIVERSITY\\n\\nDifferential Privacy 8...  \n87  alt\\nReference Models for Computer BB Oh MANNG...  \n88  ol\\nPassphrases Be) OF MANNHEIM\\n\\n—— School o...  \n89  te\\nSome Comments Be OP MANNHEIM\\n\\n—— School ...  \n90  ol\\nE-Mail Spoofing Be) OF MANNHEIM\\n——— Schoo...  \n\n[91 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Page-Text</th>\n      <th>OCR-text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Cookies\\r\\nAdvantages and Disadvantages\\r\\nAdv...</td>\n      <td>Cookies\\nAdvantages and Disadvantages\\n\\nAdvan...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Access Control\\r\\n• Controls which authenticat...</td>\n      <td>te\\nAccess Control Be OP MANNHEIM\\n\\n—— School...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XSS\\r\\n• XSS = Cross Site Scripting\\r\\n• One o...</td>\n      <td>XSS\\n\\n¢ XSS = Cross Site Scripting\\n* One of ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Javascript\\r\\nAbilities\\r\\n• Runs on the clien...</td>\n      <td>ol\\nH UNIVERSITY\\nJavascript BOR MANNHEIM\\n———...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Technique\\r\\n• Recall that e-mail communicatio...</td>\n      <td>te\\nae UNIVERSITY\\n2) OF MANNHEIM\\n\\n— School ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>Differential Privacy\\r\\nIntuition\\r\\n• Assume ...</td>\n      <td>te\\nGee 5 UNIVERSITY\\n\\nDifferential Privacy 8...</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>Reference Models for Computer \\r\\nNetworks\\r\\n...</td>\n      <td>alt\\nReference Models for Computer BB Oh MANNG...</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>Passphrases\\r\\n• Good method: choose a (silly)...</td>\n      <td>ol\\nPassphrases Be) OF MANNHEIM\\n\\n—— School o...</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>Some Comments\\r\\n• RBAC can be based on access...</td>\n      <td>te\\nSome Comments Be OP MANNHEIM\\n\\n—— School ...</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>E-Mail Spoofing\\r\\n• Creation of email message...</td>\n      <td>ol\\nE-Mail Spoofing Be) OF MANNHEIM\\n——— Schoo...</td>\n    </tr>\n  </tbody>\n</table>\n<p>91 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T20:57:27.853867Z",
     "start_time": "2023-07-15T20:57:27.849595Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Engineering\n",
    "Having prepared everything it is possible to start with prompt engineering. It is started with simple prompts and continued with more complex prompts.\n",
    "\n",
    "| **#** | **Prompt**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
    "|-------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1     | Generate a question in a flashcard style for the content delimited by triple backticks. ```{row['Page-Text']}```                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
    "| 2     | Generate a question in a flashcard style for the content delimited by triple backticks. Take into account how exam questions are normally formulated and formulate the question accordingly. ```{row['Page-Text']}```                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
    "| 3     | Generate a question in a flashcard style for the content delimited by triple backticks. When there are examples do not focus on their specifics but try to cover the overarching concept or idea. ```{row['Page-Text']}```                                                                                                                                                                                                                                                                                                                                                                                             |\n",
    "| 4     | Generate a question in a flashcard style for the content delimited by triple backticks. Focus on concepts, definitions and key-words. Take into account how exam questions are normally formulated and formulate the question accordingly. When there are examples do not focus on their specifics but try to cover the overarching concept or idea. ```{row['Page-Text']}```                                                                                                                                                                                                                                          |\n",
    "| 5     | You are a bot to support in the generation of flashcards from lecture slides. You are provided with two inputs. The first input delimited by triple backticks is the text that is copied from the slides. The second input delimited by triple quotation marks is retrieved with an OCR tool to extract all text from a slide. Follow the below process: 1. Step: Compare the first input with the second input to retrieve the relevant information 2. Step: Generate a question for this information in a flashcard style Only return the generated question. ```{row['Page-Text']}``` \\\"\\\"\\\"{row['OCR-text']}\\\"\\\"\\\" |\n",
    "| 6     | Generate a question in a flashcard style for the content delimited by triple backticks. ```{row['Page-Text']}``` Follow a similar style for generating the question as in this two examples: 1) Input: {goldstandard_train_val.loc[0, 'Page-Text']}, question: {goldstandard_train_val.loc[0, 'Question']} 2) Input: {goldstandard_train_val.loc[1, 'Page-Text']}, question: {goldstandard_train_val.loc[1, 'Question']}                                                                                                                                                                                               |\n",
    "| 7     | Generate a question in a flashcard style for the content delimited by triple backticks. Take into account how exam questions are normally formulated and formulate the question accordingly. ```{row['Page-Text']}``` Follow a similar style for generating the question as in this two examples: 1) Input: {goldstandard_train_val.loc[0, 'Page-Text']}, question: {goldstandard_train_val.loc[0, 'Question']} 2) Input: {goldstandard_train_val.loc[1, 'Page-Text']}, question: {goldstandard_train_val.loc[1, 'Question']}                                                                                          |\n",
    "\n",
    "## Zero-Shot Prompting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "refs = []\n",
    "for i, q in references.iterrows():\n",
    "    refs.append((i, [q.item()]))\n",
    "refs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T21:05:38.655472Z",
     "start_time": "2023-07-15T21:05:38.649745Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prompt 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#model_results = []\n",
    "# the chatGPT API is called and results are stored\n",
    "for index, row in content.iterrows():\n",
    "    if index >=78:\n",
    "        prompt = f\"\"\"\n",
    "        Generate a question in a flashcard style for the content delimited by triple backticks.\n",
    "        ```{row['Page-Text']}```\n",
    "        \"\"\"\n",
    "        question = chat_gpt(prompt)\n",
    "        model_results.append((index, [question]))\n",
    "        print(\"Generated question for index \", index, \": \", question)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "print(model_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T21:07:33.824595Z",
     "start_time": "2023-07-15T21:07:25.869519Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Save model_results to disk\n",
    "df_model_results = pd.DataFrame(model_results, columns=[\"Index\", \"Question\"])\n",
    "df_model_results.to_csv(\"./model_results/prompt1.csv\", index=False)\n",
    "\n",
    "# restore model_results\n",
    "df_model_results = pd.read_csv(\"./model_results/prompt1.csv\")\n",
    "model_results = [(row['Index'], [row['Question']]) for _, row in df_model_results.iterrows()]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/I516258/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PUNCTUATION  MISC  GRAMMAR  rouge1_fmeasure  \\\n",
      "ChatGPT-Prompt1           15     1        1         0.445543   \n",
      "\n",
      "                 rouge1_precision  rouge1_recall  rouge2_fmeasure  \\\n",
      "ChatGPT-Prompt1           0.38389        0.60203         0.245597   \n",
      "\n",
      "                 rouge2_precision  rouge2_recall  rougeL_fmeasure  ...  min_r  \\\n",
      "ChatGPT-Prompt1          0.208953       0.348498         0.421388  ...    0.0   \n",
      "\n",
      "                  avg_f1  max_f1  min_f1  avg_cos_sim  max_cos_sim  \\\n",
      "ChatGPT-Prompt1  0.45029     1.0     0.0     0.681447     0.999843   \n",
      "\n",
      "                 min_cos_sim  avg_sem_meteor  max_sem_meteor  min_sem_meteor  \n",
      "ChatGPT-Prompt1     0.200163        0.501238        0.952193        0.073529  \n",
      "\n",
      "[1 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Performance is evaluated\n",
    "metrics = Metrics(save_to_file=True)\n",
    "result = pd.DataFrame(\n",
    "    metrics.evaluate(model_output=model_results, references=refs),\n",
    "    index=[\"ChatGPT-Prompt1\"]\n",
    ")\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prompt 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#model_results = []\n",
    "# the chatGPT API is called and results are stored\n",
    "for index, row in content.iterrows():\n",
    "    if index >=89:\n",
    "        prompt = f\"\"\"\n",
    "        Generate a question in a flashcard style for the content delimited by triple backticks.\n",
    "        Take into account how exam questions are normally formulated and formulate the question accordingly.\n",
    "        ```{row['Page-Text']}```\n",
    "        \"\"\"\n",
    "        question = chat_gpt(prompt)\n",
    "        model_results.append((index, [question]))\n",
    "        print(\"Generated question for index \", index, \": \", question)\n",
    "        time.sleep(1)\n",
    "\n",
    "print(model_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Save model_results to disk\n",
    "df_model_results = pd.DataFrame(model_results, columns=[\"Index\", \"Question\"])\n",
    "df_model_results.to_csv(\"./model_results/prompt2.csv\", index=False)\n",
    "\n",
    "# # restore model_results\n",
    "# df_model_results = pd.read_csv(\"./model_results/prompt2.csv\")\n",
    "# model_results = [(row[\"Index\"], [row[\"Question\"]]) for _, row in df_model_results.iterrows()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/I516258/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PUNCTUATION  MISC  GRAMMAR  rouge1_fmeasure  \\\n",
      "ChatGPT-Prompt2           14     1        1         0.424193   \n",
      "\n",
      "                 rouge1_precision  rouge1_recall  rouge2_fmeasure  \\\n",
      "ChatGPT-Prompt2          0.357888       0.618192         0.237488   \n",
      "\n",
      "                 rouge2_precision  rouge2_recall  rougeL_fmeasure  ...  min_r  \\\n",
      "ChatGPT-Prompt2          0.198803       0.360979         0.399191  ...    0.0   \n",
      "\n",
      "                  avg_f1  max_f1  min_f1  avg_cos_sim  max_cos_sim  \\\n",
      "ChatGPT-Prompt2  0.41543     1.0     0.0     0.667354     0.999843   \n",
      "\n",
      "                 min_cos_sim  avg_sem_meteor  max_sem_meteor  min_sem_meteor  \n",
      "ChatGPT-Prompt2     0.121115        0.498779          0.9995        0.073529  \n",
      "\n",
      "[1 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Performance is evaluated\n",
    "metrics = Metrics(save_to_file=True)\n",
    "result = pd.DataFrame(\n",
    "    metrics.evaluate(model_output=model_results, references=refs),\n",
    "    index=[\"ChatGPT-Prompt2\"]\n",
    ")\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prompt 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_results = []\n",
    "# the chatGPT API is called and results are stored\n",
    "for index, row in content.iterrows():\n",
    "    if (index == 3):\n",
    "        break\n",
    "    prompt = f\"\"\"\n",
    "    Generate a question in a flashcard style for the content delimited by triple backticks.\n",
    "    When there are examples do not focus on their specifics but try to cover the overarching concept or idea.\n",
    "    ```{row['Page-Text']}```\n",
    "    \"\"\"\n",
    "    model_results.append((index, [chat_gpt(prompt)]))\n",
    "\n",
    "print(model_results)\n",
    "# Performance is evaluated\n",
    "metrics = Metrics(save_to_file=True)\n",
    "result = pd.DataFrame(\n",
    "    metrics.evaluate(model_output=model_results, references=refs[:3]),\n",
    "    index=[\"ChatGPT-Prompt3\"]\n",
    ")\n",
    "result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_results = []\n",
    "# the chatGPT API is called and results are stored\n",
    "for index, row in content.iterrows():\n",
    "    if (index == 3):\n",
    "        break\n",
    "    prompt = f\"\"\"\n",
    "    Generate a question in a flashcard style for the content delimited by triple backticks.\n",
    "    Focus on concepts, definitions and key-words.\n",
    "    Take into account how exam questions are normally formulated and formulate the question accordingly.\n",
    "    When there are examples do not focus on their specifics but try to cover the overarching concept or idea.\n",
    "    ```{row['Page-Text']}```\n",
    "    \"\"\"\n",
    "    model_results.append((index, [chat_gpt(prompt)]))\n",
    "\n",
    "print(model_results)\n",
    "# Performance is evaluated\n",
    "metrics = Metrics(save_to_file=True)\n",
    "result = pd.DataFrame(\n",
    "    metrics.evaluate(model_output=model_results, references=refs[:3]),\n",
    "    index=[\"ChatGPT-Prompt4\"]\n",
    ")\n",
    "result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prompt 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_results = []\n",
    "# the chatGPT API is called and results are stored\n",
    "for index, row in content.iterrows():\n",
    "    if (index == 3):\n",
    "        break\n",
    "    prompt = f\"\"\"\n",
    "    You are a bot to support in the generation of flashcards from lecture slides.\n",
    "    You are provided with two inputs. The first input delimited by triple backticks is the text that is copied from the slides.\n",
    "    The second input delimited by triple quotation marks is retrieved with an OCR tool to extract all text from a slide.\n",
    "    Follow the below process:\n",
    "    1. Step: Compare the first input with the second input to retrieve the relevant information\n",
    "    2. Step: Generate a question for this information in a flashcard style\n",
    "    Only return the generated question.\n",
    "    ```{row['Page-Text']}```\n",
    "    \\\"\\\"\\\"{row['OCR-text']}\\\"\\\"\\\"\n",
    "    \"\"\"\n",
    "    model_results.append((index, [chat_gpt(prompt)]))\n",
    "\n",
    "\n",
    "print(model_results)\n",
    "# Performance is evaluated\n",
    "metrics = Metrics(save_to_file=True)\n",
    "result = pd.DataFrame(\n",
    "    metrics.evaluate(model_output=model_results, references=refs[:3]),\n",
    "    index=[\"ChatGPT-Prompt5\"]\n",
    ")\n",
    "result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Few-Shot-Prompting\n",
    "Prompt 6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_results = []\n",
    "# the chatGPT API is called and results are stored\n",
    "for index, row in content.iterrows():\n",
    "    if index >=0:\n",
    "        prompt = f\"\"\"\n",
    "        Generate a question in a flashcard style for the content delimited by triple backticks.\n",
    "        ```{row['Page-Text']}```\n",
    "        Follow a similar style for generating the question as in this two examples:\n",
    "        1) Input: {goldstandard_train_val.loc[0, 'Page-Text']}, question: {goldstandard_train_val.loc[0, 'Question']}\n",
    "        2) Input: {goldstandard_train_val.loc[1, 'Page-Text']}, question: {goldstandard_train_val.loc[1, 'Question']}\n",
    "        \"\"\"\n",
    "        question = chat_gpt(prompt)\n",
    "        model_results.append((index, [question]))\n",
    "        print(\"Generated question for index \", index, \": \", question)\n",
    "        time.sleep(1)\n",
    "\n",
    "print(model_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Save model_results to disk\n",
    "df_model_results = pd.DataFrame(model_results, columns=[\"Index\", \"Question\"])\n",
    "df_model_results.to_csv(\"./model_results/prompt6.csv\", index=False)\n",
    "\n",
    "# # restore model_results\n",
    "# df_model_results = pd.read_csv(\"./model_results/prompt6.csv\")\n",
    "# model_results = [(row[\"Index\"], [row[\"Question\"]]) for _, row in df_model_results.iterrows()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/I516258/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PUNCTUATION  CASING  MISC  TYPOGRAPHY  rouge1_fmeasure  \\\n",
      "ChatGPT-Prompt6           10       4     1           1         0.440566   \n",
      "\n",
      "                 rouge1_precision  rouge1_recall  rouge2_fmeasure  \\\n",
      "ChatGPT-Prompt6          0.386222       0.598951         0.249332   \n",
      "\n",
      "                 rouge2_precision  rouge2_recall  ...  min_r   avg_f1  max_f1  \\\n",
      "ChatGPT-Prompt6          0.214606       0.356362  ...    0.0  0.45615     1.0   \n",
      "\n",
      "                 min_f1  avg_cos_sim  max_cos_sim  min_cos_sim  \\\n",
      "ChatGPT-Prompt6     0.0     0.698805     0.996389     0.096924   \n",
      "\n",
      "                 avg_sem_meteor  max_sem_meteor  min_sem_meteor  \n",
      "ChatGPT-Prompt6        0.503604        0.979938        0.073529  \n",
      "\n",
      "[1 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Performance is evaluated\n",
    "metrics = Metrics(save_to_file=True)\n",
    "result = pd.DataFrame(\n",
    "    metrics.evaluate(model_output=model_results, references=refs),\n",
    "    index=[\"ChatGPT-Prompt6\"]\n",
    ")\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prompt 7"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_results = []\n",
    "# the chatGPT API is called and results are stored\n",
    "for index, row in content.iterrows():\n",
    "    if index >=0:\n",
    "        prompt = f\"\"\"\n",
    "        Generate a question in a flashcard style for the content delimited by triple backticks.\n",
    "        Take into account how exam questions are normally formulated and formulate the question accordingly.\n",
    "        ```{row['Page-Text']}```\n",
    "        Follow a similar style for generating the question as in this two examples:\n",
    "        1) Input: {goldstandard_train_val.loc[0, 'Page-Text']}, question: {goldstandard_train_val.loc[0, 'Question']}\n",
    "        2) Input: {goldstandard_train_val.loc[1, 'Page-Text']}, question: {goldstandard_train_val.loc[1, 'Question']}\n",
    "        \"\"\"\n",
    "        question = chat_gpt(prompt)\n",
    "        model_results.append((index, [question]))\n",
    "        print(\"Generated question for index \", index, \": \", question)\n",
    "        time.sleep(1)\n",
    "\n",
    "print(model_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Save model_results to disk\n",
    "df_model_results = pd.DataFrame(model_results, columns=[\"Index\", \"Question\"])\n",
    "df_model_results.to_csv(\"./model_results/prompt7.csv\", index=False)\n",
    "\n",
    "# # restore model_results\n",
    "# df_model_results = pd.read_csv(\"./model_results/prompt7.csv\")\n",
    "# model_results = [(row[\"Index\"], [row[\"Question\"]]) for _, row in df_model_results.iterrows()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/I516258/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PUNCTUATION  GRAMMAR  CASING  MISC  rouge1_fmeasure  \\\n",
      "ChatGPT-Prompt7           17        1       1     1         0.435454   \n",
      "\n",
      "                 rouge1_precision  rouge1_recall  rouge2_fmeasure  \\\n",
      "ChatGPT-Prompt7          0.374773       0.610946          0.23359   \n",
      "\n",
      "                 rouge2_precision  rouge2_recall  ...  min_r    avg_f1  \\\n",
      "ChatGPT-Prompt7          0.198278       0.350348  ...    0.0  0.436784   \n",
      "\n",
      "                 max_f1  min_f1  avg_cos_sim  max_cos_sim  min_cos_sim  \\\n",
      "ChatGPT-Prompt7     1.0     0.0     0.689269     0.991066     0.060811   \n",
      "\n",
      "                 avg_sem_meteor  max_sem_meteor  min_sem_meteor  \n",
      "ChatGPT-Prompt7        0.496503         0.96699        0.073529  \n",
      "\n",
      "[1 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Performance is evaluated\n",
    "metrics = Metrics(save_to_file=True)\n",
    "result = pd.DataFrame(\n",
    "    metrics.evaluate(model_output=model_results, references=refs),\n",
    "    index=[\"ChatGPT-Prompt7\"]\n",
    ")\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
