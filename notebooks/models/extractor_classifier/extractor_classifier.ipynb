{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Set the device\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device =  \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\" # Use M1 Mac GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExtractorClassifierDataset(Dataset):\n",
    "    def __init__(self, data, resize_shape = (256,256)):\n",
    "        self.data = data\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.resize_image = transforms.Resize(resize_shape, antialias=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self. data[idx]\n",
    "        image_tensor = self.to_tensor(image)\n",
    "        image_tensor = self.resize_image(image_tensor)\n",
    "        image_tensor = image_tensor/255\n",
    "        label_tensor = torch.zeros(2)\n",
    "        label_tensor[label] = 1\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "data_dir = \"../../../datasets/extractor_classifier\"\n",
    "\n",
    "train_list = []\n",
    "validation_list = []\n",
    "test_list = []\n",
    "\n",
    "for class_name in (\"relevant\", \"not_relevant\"):\n",
    "    if class_name == \"relevant\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "\n",
    "    for image in os.listdir(f\"{data_dir}/train/{class_name}\"):\n",
    "        if image != \".ipynb_checkpoints\":\n",
    "            src_path = f\"{data_dir}/train/{class_name}/{image}\"\n",
    "            train_list.append((Image.open(src_path), label))\n",
    "\n",
    "    for image in os.listdir(f\"{data_dir}/validation/{class_name}\"):\n",
    "        if image != \".ipynb_checkpoints\":\n",
    "            src_path = f\"{data_dir}/validation/{class_name}/{image}\"\n",
    "            validation_list.append((Image.open(src_path), label))\n",
    "\n",
    "    for image in os.listdir(f\"{data_dir}/test/{class_name}\"):\n",
    "        if image != \".ipynb_checkpoints\":\n",
    "            src_path = f\"{data_dir}/test/{class_name}/{image}\"\n",
    "            test_list.append((Image.open(src_path), label))\n",
    "\n",
    "# Define the train/validation/test datasets\n",
    "train_data = ExtractorClassifierDataset(train_list)\n",
    "validation_data = ExtractorClassifierDataset(validation_list)\n",
    "test_data = ExtractorClassifierDataset(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dataloaders for each dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, )\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "class Resnet50Model(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super(Resnet50Model, self).__init__()\n",
    "        self.resnet_model = resnet50(pretrained=pretrained)\n",
    "        num_features = self.resnet_model.fc.in_features\n",
    "        self.resnet_model.fc = nn.Linear(num_features, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.resnet_model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, model_path, train_loader, validation_loader, test_loader, criterion, optimizer, epochs):\n",
    "    # Move the model to the device\n",
    "    model.to(device)\n",
    "\n",
    "    # Define variables to track the best validation accuracy and the corresponding model state\n",
    "    best_valid_f_beta = 0.0\n",
    "    train_loss_values = []\n",
    "    validation_loss_values = []\n",
    "    train_f_beta_values = []\n",
    "    validation_f_beta_values = []\n",
    "\n",
    "    train_true_labels = []\n",
    "    train_predicted_labels = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train the model for one epoch\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "\n",
    "            train_predicted_labels.extend(predicted.detach().cpu())\n",
    "            train_true_labels.extend(labels.detach().cpu())\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_loss_values.append(train_loss)\n",
    "\n",
    "        train_f_beta = fbeta_score(train_true_labels, train_predicted_labels, average=\"binary\", beta=0.5)\n",
    "        train_f_beta_values.append(train_f_beta)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f} - Train Fbeta: {train_f_beta:.4f}\")\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        validation_loss, validation_f_beta = validate_model(model, validation_loader)\n",
    "        validation_loss_values.append(validation_loss)\n",
    "        validation_f_beta_values.append(validation_f_beta)\n",
    "\n",
    "        # Save the model state if the current validation accuracy is better than the previous best\n",
    "        if validation_f_beta > best_valid_f_beta:\n",
    "            best_valid_f_beta = validation_f_beta\n",
    "            print(f\"Updated best model: epoch {epoch+1}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Load the best model state and evaluate on the test set\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "    print(\"Test model:\")\n",
    "    test_loss, test_f_beta = validate_model(model, test_loader)\n",
    "    print(f\"Test Loss: {test_loss:.4f} - Test Fbeta: {test_f_beta:.4f}\")\n",
    "\n",
    "    return model, train_loss_values, train_f_beta_values, validation_loss_values, validation_f_beta_values\n",
    "\n",
    "def validate_model(model, data_loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    valid_true_labels = []\n",
    "    valid_predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item() * images.size(0)\n",
    "\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "\n",
    "            valid_predicted_labels.extend(predicted.detach().cpu())\n",
    "            valid_true_labels.extend(labels.detach().cpu())\n",
    "\n",
    "    f_beta_score = fbeta_score(valid_true_labels, valid_predicted_labels, average=\"binary\", beta=0.5)\n",
    "    valid_loss = valid_loss / len(data_loader.dataset)\n",
    "\n",
    "    print(f\"Valid Loss: {valid_loss:.4f} - Valid Fbeta: {f_beta_score:.4f}\")\n",
    "    print(classification_report(valid_true_labels, valid_predicted_labels))\n",
    "    return valid_loss, f_beta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I518152/opt/miniconda3/envs/ankinator/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/I518152/opt/miniconda3/envs/ankinator/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model\n",
      "Validation data:\n",
      "Valid Loss: 0.3440 - Valid Fbeta: 0.8021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       208\n",
      "           1       0.86      0.64      0.73        47\n",
      "\n",
      "    accuracy                           0.91       255\n",
      "   macro avg       0.89      0.81      0.84       255\n",
      "weighted avg       0.91      0.91      0.91       255\n",
      "\n",
      "Test data:\n",
      "Valid Loss: 0.2900 - Valid Fbeta: 0.8355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       418\n",
      "           1       0.90      0.65      0.75        97\n",
      "\n",
      "    accuracy                           0.92       515\n",
      "   macro avg       0.91      0.82      0.85       515\n",
      "weighted avg       0.92      0.92      0.92       515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resnet_model = Resnet50Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet_model.parameters(), lr=LEARNING_RATE)\n",
    "model_path = \"model_results/resnet50_model_50.pt\"\n",
    "\n",
    "if os.path.isfile(model_path):\n",
    "    print(\"Evaluate model\")\n",
    "    resnet_model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "    print(\"Validation data:\")\n",
    "    validate_model(resnet_model, validation_loader)\n",
    "    print(\"Test data:\")\n",
    "    validate_model(resnet_model, test_loader)\n",
    "else:\n",
    "    print(\"Train model\")\n",
    "    model, train_loss_values, train_f_beta_values, validation_loss_values, validation_f_beta_values = train(resnet_model, model_path, train_loader, validation_loader, test_loader, criterion, optimizer, 50)\n",
    "    np.save(\"model_results/resnet50_model_50_train_loss\", train_loss_values)\n",
    "    np.save(\"model_results/resnet50_model_50_train_f_beta\", train_f_beta_values)\n",
    "    np.save(\"model_results/resnet50_model_50_valid_loss\", validation_loss_values)\n",
    "    np.save(\"model_results/resnet50_model_50_valid_f_beta\", validation_f_beta_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}