{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pypdfium2 import PdfDocument\n",
    "import pypdfium2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import random\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-03-24/lib/python3.9/site-packages (from opencv-python) (1.23.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.0.74\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell contains the code to label the dataset. The user is asked whether the page/image is relevant or not\n",
    "\n",
    "pdf_dir = \"../../../datasets/extractor_classifier/slides\" # Slides to label\n",
    "relevant_dir = \"../../../datasets/extractor_classifier/dataset_images/relevant\"\n",
    "not_relevant_dir = \"../../../datasets/extractor_classifier/dataset_images/not_relevant\"\n",
    "\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_file = open(os.path.join(pdf_dir, filename), \"rb\")\n",
    "        pdf_document = PdfDocument(pdf_file)\n",
    "\n",
    "        for page_index, page_content in enumerate(pdf_document, 0):\n",
    "            bitmap = page_content.render(scale=2)\n",
    "            page_image = bitmap.to_pil()\n",
    "            plt.imshow(page_image)\n",
    "            plt.show()\n",
    "            input_str = input(\"Is this image relevant? (y/n)\")\n",
    "\n",
    "            if input_str.lower() == \"n\":\n",
    "                image_path = os.path.join(not_relevant_dir, f\"{filename}_{page_index}.png\")\n",
    "            else:\n",
    "                image_path = os.path.join(relevant_dir, f\"{filename}_{page_index}.png\")\n",
    "            page_image.save(image_path)\n",
    "\n",
    "        pdf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF-Name</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Page Number</th>\n",
       "      <th>Marked for processing</th>\n",
       "      <th>Includes Image Data</th>\n",
       "      <th>Includes formula</th>\n",
       "      <th>Question 1</th>\n",
       "      <th>Question 2</th>\n",
       "      <th>Question 3</th>\n",
       "      <th>Title of the slide</th>\n",
       "      <th>Type of Question</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ase_combined.pdf</td>\n",
       "      <td>Agile Software Engineering</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ase_combined.pdf</td>\n",
       "      <td>Agile Software Engineering</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ase_combined.pdf</td>\n",
       "      <td>Agile Software Engineering</td>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ase_combined.pdf</td>\n",
       "      <td>Agile Software Engineering</td>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ase_combined.pdf</td>\n",
       "      <td>Agile Software Engineering</td>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cost of Software Failures</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PDF-Name                       Topic  Page Number  \\\n",
       "0  ase_combined.pdf  Agile Software Engineering            1   \n",
       "1  ase_combined.pdf  Agile Software Engineering            2   \n",
       "2  ase_combined.pdf  Agile Software Engineering            3   \n",
       "3  ase_combined.pdf  Agile Software Engineering            4   \n",
       "4  ase_combined.pdf  Agile Software Engineering            5   \n",
       "\n",
       "  Marked for processing Includes Image Data Includes formula Question 1  \\\n",
       "0                    No                 NaN              NaN        NaN   \n",
       "1                    No                 NaN              NaN        NaN   \n",
       "2                    No                 NaN              NaN        NaN   \n",
       "3                    No                 NaN              NaN        NaN   \n",
       "4                    No                  No               No        NaN   \n",
       "\n",
       "  Question 2 Question 3         Title of the slide Type of Question Comment  \n",
       "0        NaN        NaN                        NaN              NaN     NaN  \n",
       "1        NaN        NaN                        NaN              NaN     NaN  \n",
       "2        NaN        NaN                        NaN              NaN     NaN  \n",
       "3        NaN        NaN                        NaN              NaN     NaN  \n",
       "4        NaN        NaN  Cost of Software Failures              NaN     NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset from new master label data\n",
    "ankinator_master_data = pd.read_csv(\"../../../datasets/anki_data/Ankinator_Master_Labeling.csv\", delimiter=\";\")\n",
    "ankinator_master_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame length: 5739\n",
      "DataFrame length: 3482\n"
     ]
    }
   ],
   "source": [
    "data_length = len(ankinator_master_data)\n",
    "print(\"DataFrame length:\", data_length)\n",
    "ankinator_master_data.dropna(subset=[\"Marked for processing\"], inplace=True)\n",
    "data_length = len(ankinator_master_data)\n",
    "print(\"DataFrame length:\", data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts:\n",
      "Yes    2181\n",
      "No     1301\n",
      "Name: Marked for processing, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "Yes    62.636416\n",
      "No     37.363584\n",
      "Name: Marked for processing, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "counts = ankinator_master_data[\"Marked for processing\"].value_counts()\n",
    "\n",
    "total_count = counts.sum()\n",
    "percentage = (counts / total_count) * 100\n",
    "\n",
    "print(\"Counts:\")\n",
    "print(counts)\n",
    "print(\"\\nPercentages:\")\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_dir = \"../../../datasets/anki_data/pdf_files\"\n",
    "relevant_dir_image = \"../../../datasets/extractor_classifier/dataset_images/relevant\"\n",
    "not_relevant_dir_image = \"../../../datasets/extractor_classifier/dataset_images/not_relevant\"\n",
    "\n",
    "for index, row in ankinator_master_data.iterrows():\n",
    "    pdf_filename = os.path.join(pdf_dir, row['PDF-Name'])\n",
    "    page_number = int(row['Page Number'])\n",
    "    marked_for_processing = row['Marked for processing']\n",
    "\n",
    "    pdf_file = open(pdf_filename, 'rb')\n",
    "    pdf_document = pypdfium2.PdfDocument(pdf_file)\n",
    "    page = pdf_document.get_page(page_number - 1)  # Pages are 0-indexed\n",
    "\n",
    "    bitmap = page.render(scale=1)\n",
    "    image = bitmap.to_pil()\n",
    "\n",
    "    if marked_for_processing == \"Yes\":\n",
    "         image_path = os.path.join(relevant_dir_image, f\"{row['PDF-Name']}_page_{page_number}.png\")\n",
    "    else:\n",
    "        image_path = os.path.join(not_relevant_dir_image, f\"{row['PDF-Name']}_page_{page_number}.png\")\n",
    "\n",
    "    image.save(image_path)\n",
    "    pdf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create train, validation and test data from dataset\n",
    "# Only do this once\n",
    "\n",
    "root_dir = \"../../../datasets/extractor_classifier/dataset_images/\"\n",
    "\n",
    "# Define the percentage of data to use for each set\n",
    "train_percent = 0.7\n",
    "val_percent = 0.10\n",
    "test_percent = 0.20\n",
    "\n",
    "# Create a list of class names (assumes each class is a subfolder of root_dir)\n",
    "class_names = sorted(os.listdir(root_dir))\n",
    "\n",
    "if \".DS_Store\" in class_names:\n",
    "    class_names.remove(\".DS_Store\")\n",
    "\n",
    "# Define the output directories for the saved datasets\n",
    "train_output_dir = \"../../../datasets/extractor_classifier/train/\"\n",
    "val_output_dir = \"../../../datasets/extractor_classifier/validation/\"\n",
    "test_output_dir = \"../../../datasets/extractor_classifier/test/\"\n",
    "\n",
    "# Create the output directories if they don't already exist\n",
    "os.makedirs(train_output_dir, exist_ok=True)\n",
    "os.makedirs(val_output_dir, exist_ok=True)\n",
    "os.makedirs(test_output_dir, exist_ok=True)\n",
    "\n",
    "# Create train, validation, and test list\n",
    "train_list = []\n",
    "validation_list = []\n",
    "test_list = []\n",
    "\n",
    "# Split the data for each class into train, validation, and test sets\n",
    "for class_name in class_names:\n",
    "    # Get a list of all images for this class\n",
    "    images = os.listdir(root_dir + class_name)\n",
    "    random.Random(42).shuffle(images)\n",
    "\n",
    "    # Split the images into train, validation, and test sets\n",
    "    num_images = len(images)\n",
    "    num_train = int(train_percent * num_images)\n",
    "    num_val = int(val_percent * num_images)\n",
    "\n",
    "    train_images = images[:num_train]\n",
    "    val_images = images[num_train:num_train+num_val]\n",
    "    test_images = images[num_train+num_val:]\n",
    "\n",
    "    for image in train_images:\n",
    "        if image != \".ipynb_checkpoints\":\n",
    "            src_path = root_dir + class_name + \"/\" + image\n",
    "            label = class_names.index(class_name)\n",
    "            train_list.append((Image.open(src_path), label))\n",
    "\n",
    "    for image in val_images:\n",
    "        if image != \".ipynb_checkpoints\":\n",
    "            src_path = root_dir + class_name + \"/\" + image\n",
    "            label = class_names.index(class_name)\n",
    "            validation_list.append((Image.open(src_path), label))\n",
    "\n",
    "    for image in test_images:\n",
    "        if image != \".ipynb_checkpoints\":\n",
    "            src_path = root_dir + class_name + \"/\" + image\n",
    "            label = class_names.index(class_name)\n",
    "            test_list.append((Image.open(src_path), label))\n",
    "\n",
    "# Save the train dataset\n",
    "for image, label in train_list:\n",
    "    class_name = class_names[label]\n",
    "    output_path = os.path.join(train_output_dir, class_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    image_filename = os.path.splitext(os.path.basename(image.filename))[0] + \".jpg\"\n",
    "    shutil.copyfile(image.filename, os.path.join(output_path, image_filename))\n",
    "\n",
    "# Save the validation dataset\n",
    "for image, label in validation_list:\n",
    "    class_name = class_names[label]\n",
    "    output_path = os.path.join(val_output_dir, class_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    image_filename = os.path.splitext(os.path.basename(image.filename))[0] + \".jpg\"\n",
    "    shutil.copyfile(image.filename, os.path.join(output_path, image_filename))\n",
    "\n",
    "# Save the test dataset\n",
    "for image, label in test_list:\n",
    "    class_name = class_names[label]\n",
    "    output_path = os.path.join(test_output_dir, class_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    image_filename = os.path.splitext(os.path.basename(image.filename))[0] + \".jpg\"\n",
    "    shutil.copyfile(image.filename, os.path.join(output_path, image_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1527/1527 [00:49<00:00, 30.60it/s]\n",
      "100%|██████████| 910/910 [00:29<00:00, 30.72it/s]\n",
      "100%|██████████| 1527/1527 [00:20<00:00, 74.14it/s]\n",
      "100%|██████████| 910/910 [00:11<00:00, 78.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data augmentation\n",
    "\n",
    "\n",
    "\n",
    "# Path to the folder containing the images\n",
    "data_dir = \"../../../datasets/extractor_classifier/\"\n",
    "\n",
    "# Define the directories where the images are stored\n",
    "relevant_images_dir = os.path.join(data_dir, \"train/relevant\")\n",
    "not_relevant_images_dir = os.path.join(data_dir, \"train/not_relevant\")\n",
    "\n",
    "# Create a new directory to store the augmented images\n",
    "train_data_augmentation_dir = \"train_data_augmentation\"\n",
    "\n",
    "# Path to the output folder for augmented images\n",
    "train_data_augmentation_dir = os.path.join(data_dir, \"train_data_augmentation\")\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(train_data_augmentation_dir):\n",
    "    os.makedirs(train_data_augmentation_dir)\n",
    "    os.makedirs(os.path.join(train_data_augmentation_dir, \"relevant\"))\n",
    "    os.makedirs(os.path.join(train_data_augmentation_dir, \"not_relevant\"))\n",
    "\n",
    "def blur_augmentation(image_dir, relevance):\n",
    "    for image_file in tqdm(os.listdir(image_dir)):\n",
    "        # Read the image\n",
    "        image = cv2.imread(os.path.join(image_dir, image_file))\n",
    "\n",
    "        # Apply augmentation\n",
    "        augmented_image = cv2.GaussianBlur(image, (7, 7), 0)\n",
    "\n",
    "        # Save the augmented image\n",
    "        save_path = os.path.join(train_data_augmentation_dir, f\"{relevance}/{image_file}_blur.png\")\n",
    "        cv2.imwrite(save_path, augmented_image)\n",
    "\n",
    "def add_random_boxes(img,n_k,size=32):\n",
    "    h,w = size,size\n",
    "    img = np.asarray(img)\n",
    "    img_size = img.shape[1]\n",
    "    boxes = []\n",
    "    for k in range(n_k):\n",
    "        y,x = np.random.randint(0,img_size-w,(2,))\n",
    "        img[y:y+h,x:x+w] = 0\n",
    "        boxes.append((x,y,h,w))\n",
    "    return img\n",
    "\n",
    "def noise_augmentation(image_dir, relevance):\n",
    "    for image_file in tqdm(os.listdir(image_dir)):\n",
    "        # Read the image\n",
    "        image = cv2.imread(os.path.join(image_dir, image_file))\n",
    "        noisy_image = add_random_boxes(image, 30, 128)\n",
    "\n",
    "        # Save the augmented image\n",
    "        save_path = os.path.join(train_data_augmentation_dir, f\"{relevance}/{image_file}_random_blocks.png\")\n",
    "        cv2.imwrite(save_path, noisy_image)\n",
    "\n",
    "noise_augmentation(relevant_images_dir, \"relevant\")\n",
    "noise_augmentation(not_relevant_images_dir, \"not_relevant\")\n",
    "blur_augmentation(relevant_images_dir, \"relevant\")\n",
    "blur_augmentation(not_relevant_images_dir, \"not_relevant\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
