{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from pypdfium2 import PdfDocument, PdfPage\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This cell contains the code to label the dataset. The user is asked whether the page/image is relevant or not\n",
    "\n",
    "pdf_dir = \"../../../datasets/extractor_classifier/slides\" # Slides to label\n",
    "relevant_dir = \"../../../datasets/extractor_classifier/dataset_images/relevant\"\n",
    "not_relevant_dir = \"../../../datasets/extractor_classifier/dataset_images/not_relevant\"\n",
    "\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_file = open(os.path.join(pdf_dir, filename), \"rb\")\n",
    "        pdf_document = PdfDocument(pdf_file)\n",
    "\n",
    "        for page_index, page_content in enumerate(pdf_document, 0):\n",
    "            bitmap = page_content.render(scale=2)\n",
    "            page_image = bitmap.to_pil()\n",
    "            plt.imshow(page_image)\n",
    "            plt.show()\n",
    "            input_str = input(\"Is this image relevant? (y/n)\")\n",
    "\n",
    "            if input_str.lower() == \"n\":\n",
    "                image_path = os.path.join(not_relevant_dir, f\"{filename}_{page_index}.png\")\n",
    "            else:\n",
    "                image_path = os.path.join(relevant_dir, f\"{filename}_{page_index}.png\")\n",
    "            page_image.save(image_path)\n",
    "\n",
    "        pdf_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Create train, validation and test data from dataset\n",
    "# Only do this once\n",
    "\n",
    "root_dir = \"../../../datasets/extractor_classifier/dataset_images/\"\n",
    "\n",
    "# Define the percentage of data to use for each set\n",
    "train_percent = 0.7\n",
    "val_percent = 0.10\n",
    "test_percent = 0.20\n",
    "\n",
    "# Create a list of class names (assumes each class is a subfolder of root_dir)\n",
    "class_names = sorted(os.listdir(root_dir))\n",
    "\n",
    "if \".DS_Store\" in class_names:\n",
    "    class_names.remove(\".DS_Store\")\n",
    "\n",
    "# Define the output directories for the saved datasets\n",
    "train_output_dir = \"../../../datasets/extractor_classifier/train/\"\n",
    "val_output_dir = \"../../../datasets/extractor_classifier/validation/\"\n",
    "test_output_dir = \"../../../datasets/extractor_classifier/test/\"\n",
    "\n",
    "# Create the output directories if they don't already exist\n",
    "os.makedirs(train_output_dir, exist_ok=True)\n",
    "os.makedirs(val_output_dir, exist_ok=True)\n",
    "os.makedirs(test_output_dir, exist_ok=True)\n",
    "\n",
    "# Create train, validation, and test list\n",
    "train_list = []\n",
    "validation_list = []\n",
    "test_list = []\n",
    "\n",
    "# Split the data for each class into train, validation, and test sets\n",
    "for class_name in class_names:\n",
    "    # Get a list of all images for this class\n",
    "    images = os.listdir(root_dir + class_name)\n",
    "    random.Random(42).shuffle(images)\n",
    "\n",
    "    # Split the images into train, validation, and test sets\n",
    "    num_images = len(images)\n",
    "    num_train = int(train_percent * num_images)\n",
    "    num_val = int(val_percent * num_images)\n",
    "\n",
    "    train_images = images[:num_train]\n",
    "    val_images = images[num_train:num_train+num_val]\n",
    "    test_images = images[num_train+num_val:]\n",
    "\n",
    "    for image in train_images:\n",
    "        src_path = root_dir + class_name + \"/\" + image\n",
    "        label = class_names.index(class_name)\n",
    "        train_list.append((Image.open(src_path), label))\n",
    "\n",
    "    for image in val_images:\n",
    "        src_path = root_dir + class_name + \"/\" + image\n",
    "        label = class_names.index(class_name)\n",
    "        validation_list.append((Image.open(src_path), label))\n",
    "\n",
    "    for image in test_images:\n",
    "        src_path = root_dir + class_name + \"/\" + image\n",
    "        label = class_names.index(class_name)\n",
    "        test_list.append((Image.open(src_path), label))\n",
    "\n",
    "# Save the train dataset\n",
    "for image, label in train_list:\n",
    "    class_name = class_names[label]\n",
    "    output_path = os.path.join(train_output_dir, class_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    image_filename = os.path.splitext(os.path.basename(image.filename))[0] + \".jpg\"\n",
    "    shutil.copyfile(image.filename, os.path.join(output_path, image_filename))\n",
    "\n",
    "# Save the validation dataset\n",
    "for image, label in validation_list:\n",
    "    class_name = class_names[label]\n",
    "    output_path = os.path.join(val_output_dir, class_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    image_filename = os.path.splitext(os.path.basename(image.filename))[0] + \".jpg\"\n",
    "    shutil.copyfile(image.filename, os.path.join(output_path, image_filename))\n",
    "\n",
    "# Save the test dataset\n",
    "for image, label in test_list:\n",
    "    class_name = class_names[label]\n",
    "    output_path = os.path.join(test_output_dir, class_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    image_filename = os.path.splitext(os.path.basename(image.filename))[0] + \".jpg\"\n",
    "    shutil.copyfile(image.filename, os.path.join(output_path, image_filename))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1458/1458 [00:47<00:00, 30.61it/s]\n",
      "100%|██████████| 333/333 [00:14<00:00, 23.69it/s]\n",
      "100%|██████████| 1458/1458 [00:49<00:00, 29.66it/s]\n",
      "100%|██████████| 333/333 [00:15<00:00, 21.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "\n",
    "# Path to the folder containing the images\n",
    "data_dir = \"../../../datasets/extractor_classifier/\"\n",
    "\n",
    "# Define the directories where the images are stored\n",
    "relevant_images_dir = os.path.join(data_dir, \"train/relevant\")\n",
    "not_relevant_images_dir = os.path.join(data_dir, \"train/not_relevant\")\n",
    "\n",
    "# Create a new directory to store the augmented images\n",
    "train_data_augmentation_dir = \"train_data_augmentation\"\n",
    "\n",
    "# Path to the output folder for augmented images\n",
    "train_data_augmentation_dir = os.path.join(data_dir, \"train_data_augmentation\")\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(train_data_augmentation_dir):\n",
    "    os.makedirs(train_data_augmentation_dir)\n",
    "    os.makedirs(os.path.join(train_data_augmentation_dir, \"relevant\"))\n",
    "    os.makedirs(os.path.join(train_data_augmentation_dir, \"not_relevant\"))\n",
    "\n",
    "def blur_augmentation(image_dir, relevance):\n",
    "    for image_file in tqdm(os.listdir(image_dir)):\n",
    "        image = cv2.imread(os.path.join(image_dir, image_file))\n",
    "        augmented_image = cv2.GaussianBlur(image, (7, 7), 0)\n",
    "        save_path = os.path.join(train_data_augmentation_dir, f\"{relevance}/{image_file}_blur.png\")\n",
    "        cv2.imwrite(save_path, augmented_image)\n",
    "\n",
    "def add_random_boxes(img,n_k,size=32):\n",
    "    h,w = size,size\n",
    "    img = np.asarray(img)\n",
    "    img_size = img.shape[1]\n",
    "    boxes = []\n",
    "    for k in range(n_k):\n",
    "        y,x = np.random.randint(0,img_size-w,(2,))\n",
    "        img[y:y+h,x:x+w] = 0\n",
    "        boxes.append((x,y,h,w))\n",
    "    return img\n",
    "\n",
    "def noise_augmentation(image_dir, relevance):\n",
    "    for image_file in tqdm(os.listdir(image_dir)):\n",
    "        image = cv2.imread(os.path.join(image_dir, image_file))\n",
    "        noisy_image = add_random_boxes(image, 30, 128)\n",
    "        save_path = os.path.join(train_data_augmentation_dir, f\"{relevance}/{image_file}_random_blocks.png\")\n",
    "        cv2.imwrite(save_path, noisy_image)\n",
    "\n",
    "noise_augmentation(relevant_images_dir, \"relevant\")\n",
    "noise_augmentation(not_relevant_images_dir, \"not_relevant\")\n",
    "blur_augmentation(relevant_images_dir, \"relevant\")\n",
    "blur_augmentation(not_relevant_images_dir, \"not_relevant\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PDFs for text classifier\n",
    "import os\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfWriter, PdfReader\n",
    "\n",
    "# Set the paths to the image and PDF directories\n",
    "image_directory = '../../../datasets/extractor_classifier'\n",
    "pdf_directory = '../../../datasets/extractor_classifier/slides'\n",
    "\n",
    "# Iterate over the image files in the directory\n",
    "for directory in [\"train/relevant\", \"train/not_relevant\", \"validation/relevant\", \"validation/not_relevant\", \"test/relevant\", \"test/not_relevant\"]:\n",
    "    os.makedirs(f\"../../../datasets/extractor_classifier/pdf_documents/{directory}\", exist_ok=True)\n",
    "    for image_filename in os.listdir(os.path.join(image_directory, directory)):\n",
    "        image_filename = os.path.join(directory, image_filename)\n",
    "\n",
    "        if image_filename.endswith('.jpg'):\n",
    "            # Extract the document name and page number from the image filename\n",
    "            document_name, page_number = os.path.splitext(image_filename)[0].rsplit('_', 1)\n",
    "            document_name = document_name.split(\"/\")[2]\n",
    "\n",
    "            pdf_path = os.path.join(pdf_directory, document_name)\n",
    "\n",
    "            # Extract the page from the PDF document\n",
    "            with open(pdf_path, 'rb') as pdf_file:\n",
    "                try:\n",
    "                    pdf_reader = PdfReader(pdf_file)\n",
    "                    page = pdf_reader.pages[int(page_number)]\n",
    "                    # Save the PDF document\n",
    "                    output_pdf_path = f\"../../../datasets/extractor_classifier/pdf_documents/{directory}/{document_name}_{page_number}.pdf\"\n",
    "                    output_pdf_writer = PdfWriter()\n",
    "                    output_pdf_writer.add_page(page)\n",
    "\n",
    "                    with open(output_pdf_path, 'wb') as output_pdf_file:\n",
    "                        output_pdf_writer.write(output_pdf_file)\n",
    "                except:\n",
    "                    print(\"Exception\")\n",
    "                    print(document_name)\n",
    "                    print(page_number)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def plain_pdf_extraction(pdf_document: PdfDocument) -> [(int, str)]:\n",
    "    pages = []\n",
    "    for page_index, page_content in enumerate(pdf_document, 0):\n",
    "        page_text: str = page_content.get_textpage().get_text_range()\n",
    "        pages.append((page_index, page_text))\n",
    "    return pages\n",
    "\n",
    "\n",
    "def extract_text(path:str) -> str:\n",
    "    pdf_document = PdfDocument(path)\n",
    "    extracted_pages = plain_pdf_extraction(pdf_document)\n",
    "    for page_index, page_text in extracted_pages:\n",
    "        return page_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1423/1423 [00:10<00:00, 133.89it/s]\n",
      "100%|██████████| 205/205 [00:01<00:00, 137.98it/s]\n",
      "100%|██████████| 407/407 [00:03<00:00, 132.02it/s]\n",
      "100%|██████████| 333/333 [00:01<00:00, 168.42it/s]\n",
      "100%|██████████| 46/46 [00:00<00:00, 104.90it/s]\n",
      "100%|██████████| 96/96 [00:00<00:00, 135.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tor\\r\\n• Initially abreviation for The Onion Router\\r\\n• Goal: Ensure private Internet communication, i.e., hide\\r\\ncommunication partners\\r\\nData Security & Privacy Tor 2', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create training, validation and test text data\n",
    "\n",
    "data_dir = \"../../../datasets/extractor_classifier/pdf_documents\"\n",
    "\n",
    "train_list = []\n",
    "validation_list = []\n",
    "test_list = []\n",
    "\n",
    "for class_name in (\"relevant\", \"not_relevant\"):\n",
    "    if class_name == \"relevant\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "\n",
    "    for image in tqdm(os.listdir(f\"{data_dir}/train/{class_name}\")):\n",
    "        if image != \".ipynb_checkpoints\":\n",
    "            try:\n",
    "                src_path = f\"{data_dir}/train/{class_name}/{image}\"\n",
    "                text = extract_text(src_path)\n",
    "                train_list.append((text, label))\n",
    "            except:\n",
    "                print(\"ERROR\")\n",
    "\n",
    "    for image in tqdm(os.listdir(f\"{data_dir}/validation/{class_name}\")):\n",
    "        if image != \".ipynb_checkpoints\":\n",
    "            try:\n",
    "                src_path = f\"{data_dir}/validation/{class_name}/{image}\"\n",
    "                text = extract_text(src_path)\n",
    "                validation_list.append((text, label))\n",
    "            except:\n",
    "                print(\"ERROR\")\n",
    "\n",
    "    for image in tqdm(os.listdir(f\"{data_dir}/test/{class_name}\")):\n",
    "        if image != \".ipynb_checkpoints\":\n",
    "            try:\n",
    "                src_path = f\"{data_dir}/test/{class_name}/{image}\"\n",
    "                text = extract_text(src_path)\n",
    "                test_list.append((text, label))\n",
    "            except:\n",
    "                print(\"ERROR\")\n",
    "print(train_list[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data, columns=['Text', 'Label'])\n",
    "    df.to_csv(filename, index=False, quoting=csv.QUOTE_NONNUMERIC, escapechar='\\\\')\n",
    "\n",
    "\n",
    "# Save each list to a separate CSV file\n",
    "save_to_csv(train_list, 'train_data_text.csv')\n",
    "save_to_csv(validation_list, 'validation_data_text.csv')\n",
    "save_to_csv(test_list, 'test_data_text.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def read_from_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    data = [(row['Text'], row['Label']) for _, row in df.iterrows()]\n",
    "    return data\n",
    "\n",
    "train_list= read_from_csv('train_data_text.csv')\n",
    "validation_list = read_from_csv('validation_data_text.csv')\n",
    "test_list = read_from_csv('test_data_text.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}