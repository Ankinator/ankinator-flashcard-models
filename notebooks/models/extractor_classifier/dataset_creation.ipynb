{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "from pypdfium2 import PdfDocument\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import random\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import Augmentor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This cell contains the code to label the dataset. The user is asked whether the page/image is relevant or not\n",
    "\n",
    "pdf_dir = \"../../../datasets/extractor_classifier/slides\" # Slides to label\n",
    "relevant_dir = \"../../../datasets/extractor_classifier/dataset_images/relevant\"\n",
    "not_relevant_dir = \"../../../datasets/extractor_classifier/dataset_images/not_relevant\"\n",
    "\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_file = open(os.path.join(pdf_dir, filename), \"rb\")\n",
    "        pdf_document = PdfDocument(pdf_file)\n",
    "\n",
    "        for page_index, page_content in enumerate(pdf_document, 0):\n",
    "            bitmap = page_content.render(scale=2)\n",
    "            page_image = bitmap.to_pil()\n",
    "            plt.imshow(page_image)\n",
    "            plt.show()\n",
    "            input_str = input(\"Is this image relevant? (y/n)\")\n",
    "\n",
    "            if input_str.lower() == \"n\":\n",
    "                image_path = os.path.join(not_relevant_dir, f\"{filename}_{page_index}.png\")\n",
    "            else:\n",
    "                image_path = os.path.join(relevant_dir, f\"{filename}_{page_index}.png\")\n",
    "            page_image.save(image_path)\n",
    "\n",
    "        pdf_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Create train, validation and test data from dataset\n",
    "# Only do this once\n",
    "\n",
    "root_dir = \"../../../datasets/extractor_classifier/dataset_images/\"\n",
    "\n",
    "# Define the percentage of data to use for each set\n",
    "train_percent = 0.7\n",
    "val_percent = 0.10\n",
    "test_percent = 0.20\n",
    "\n",
    "# Create a list of class names (assumes each class is a subfolder of root_dir)\n",
    "class_names = sorted(os.listdir(root_dir))\n",
    "\n",
    "if \".DS_Store\" in class_names:\n",
    "    class_names.remove(\".DS_Store\")\n",
    "\n",
    "# Define the output directories for the saved datasets\n",
    "train_output_dir = \"../../../datasets/extractor_classifier/train/\"\n",
    "val_output_dir = \"../../../datasets/extractor_classifier/validation/\"\n",
    "test_output_dir = \"../../../datasets/extractor_classifier/test/\"\n",
    "\n",
    "# Create the output directories if they don't already exist\n",
    "os.makedirs(train_output_dir, exist_ok=True)\n",
    "os.makedirs(val_output_dir, exist_ok=True)\n",
    "os.makedirs(test_output_dir, exist_ok=True)\n",
    "\n",
    "# Create train, validation, and test list\n",
    "train_list = []\n",
    "validation_list = []\n",
    "test_list = []\n",
    "\n",
    "# Split the data for each class into train, validation, and test sets\n",
    "for class_name in class_names:\n",
    "    # Get a list of all images for this class\n",
    "    images = os.listdir(root_dir + class_name)\n",
    "    random.Random(42).shuffle(images)\n",
    "\n",
    "    # Split the images into train, validation, and test sets\n",
    "    num_images = len(images)\n",
    "    num_train = int(train_percent * num_images)\n",
    "    num_val = int(val_percent * num_images)\n",
    "\n",
    "    train_images = images[:num_train]\n",
    "    val_images = images[num_train:num_train+num_val]\n",
    "    test_images = images[num_train+num_val:]\n",
    "\n",
    "    for image in train_images:\n",
    "        src_path = root_dir + class_name + \"/\" + image\n",
    "        label = class_names.index(class_name)\n",
    "        train_list.append((Image.open(src_path), label))\n",
    "\n",
    "    for image in val_images:\n",
    "        src_path = root_dir + class_name + \"/\" + image\n",
    "        label = class_names.index(class_name)\n",
    "        validation_list.append((Image.open(src_path), label))\n",
    "\n",
    "    for image in test_images:\n",
    "        src_path = root_dir + class_name + \"/\" + image\n",
    "        label = class_names.index(class_name)\n",
    "        test_list.append((Image.open(src_path), label))\n",
    "\n",
    "# Save the train dataset\n",
    "for image, label in train_list:\n",
    "    class_name = class_names[label]\n",
    "    output_path = os.path.join(train_output_dir, class_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    image_filename = os.path.splitext(os.path.basename(image.filename))[0] + \".jpg\"\n",
    "    shutil.copyfile(image.filename, os.path.join(output_path, image_filename))\n",
    "\n",
    "# Save the validation dataset\n",
    "for image, label in validation_list:\n",
    "    class_name = class_names[label]\n",
    "    output_path = os.path.join(val_output_dir, class_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    image_filename = os.path.splitext(os.path.basename(image.filename))[0] + \".jpg\"\n",
    "    shutil.copyfile(image.filename, os.path.join(output_path, image_filename))\n",
    "\n",
    "# Save the test dataset\n",
    "for image, label in test_list:\n",
    "    class_name = class_names[label]\n",
    "    output_path = os.path.join(test_output_dir, class_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    image_filename = os.path.splitext(os.path.basename(image.filename))[0] + \".jpg\"\n",
    "    shutil.copyfile(image.filename, os.path.join(output_path, image_filename))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1458/1458 [00:45<00:00, 32.35it/s]\n",
      "100%|██████████| 333/333 [00:14<00:00, 23.73it/s]\n",
      "100%|██████████| 1458/1458 [00:50<00:00, 29.02it/s]\n",
      "100%|██████████| 333/333 [00:14<00:00, 22.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data augmentation\n",
    "\n",
    "\n",
    "\n",
    "# Path to the folder containing the images\n",
    "data_dir = \"../../../datasets/extractor_classifier/\"\n",
    "\n",
    "# Define the directories where the images are stored\n",
    "relevant_images_dir = os.path.join(data_dir, \"train/relevant\")\n",
    "not_relevant_images_dir = os.path.join(data_dir, \"train/not_relevant\")\n",
    "\n",
    "# Create a new directory to store the augmented images\n",
    "train_data_augmentation_dir = \"train_data_augmentation\"\n",
    "\n",
    "# Path to the output folder for augmented images\n",
    "train_data_augmentation_dir = os.path.join(data_dir, \"train_data_augmentation\")\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(train_data_augmentation_dir):\n",
    "    os.makedirs(train_data_augmentation_dir)\n",
    "    os.makedirs(os.path.join(train_data_augmentation_dir, \"relevant\"))\n",
    "    os.makedirs(os.path.join(train_data_augmentation_dir, \"not_relevant\"))\n",
    "\n",
    "def blur_augmentation(image_dir, relevance):\n",
    "    for image_file in tqdm(os.listdir(image_dir)):\n",
    "        # Read the image\n",
    "        image = cv2.imread(os.path.join(image_dir, image_file))\n",
    "\n",
    "        # Apply augmentation\n",
    "        augmented_image = cv2.GaussianBlur(image, (7, 7), 0)\n",
    "\n",
    "        # Save the augmented image\n",
    "        save_path = os.path.join(train_data_augmentation_dir, f\"{relevance}/{image_file}_blur.png\")\n",
    "        cv2.imwrite(save_path, augmented_image)\n",
    "\n",
    "def add_random_boxes(img,n_k,size=32):\n",
    "    h,w = size,size\n",
    "    img = np.asarray(img)\n",
    "    img_size = img.shape[1]\n",
    "    boxes = []\n",
    "    for k in range(n_k):\n",
    "        y,x = np.random.randint(0,img_size-w,(2,))\n",
    "        img[y:y+h,x:x+w] = 0\n",
    "        boxes.append((x,y,h,w))\n",
    "    return img\n",
    "\n",
    "def noise_augmentation(image_dir, relevance):\n",
    "    for image_file in tqdm(os.listdir(image_dir)):\n",
    "        # Read the image\n",
    "        image = cv2.imread(os.path.join(image_dir, image_file))\n",
    "        noisy_image = add_random_boxes(image, 30, 128)\n",
    "\n",
    "        # Save the augmented image\n",
    "        save_path = os.path.join(train_data_augmentation_dir, f\"{relevance}/{image_file}_random_blocks.png\")\n",
    "        cv2.imwrite(save_path, noisy_image)\n",
    "\n",
    "noise_augmentation(relevant_images_dir, \"relevant\")\n",
    "noise_augmentation(not_relevant_images_dir, \"not_relevant\")\n",
    "blur_augmentation(relevant_images_dir, \"relevant\")\n",
    "blur_augmentation(not_relevant_images_dir, \"not_relevant\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}